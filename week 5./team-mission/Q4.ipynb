{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. MNIST 데이터셋을 Load -> 학습할 모델을 구현, 가중치 초기화 \n",
    "#     -> 구현한 모델을 학습시키기 위한 loss 함수 opitmizer 구현\n",
    "#     -> 구현한 함수들을 이용해 학습 Loop 구현 \n",
    "# output : 학습 Loop\n",
    "\n",
    "# A. \n",
    "#   1. optimize\n",
    "#   2. loss fn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "linear = torch.nn.Linear(784, 10, bias = True).to(device)\n",
    "\n",
    "torch.nn.init.normal_(linear.weight)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr = 0.1)\n",
    "# lr = learning rate, 미분값을 얼만큼 이동시킬 것인가를 결정 한다.\n",
    "# 초기값이 크다면 초반에는 loss값이 빠르게 줄어들지만 나중에는 underfitting이 발생하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(training_epochs):\n",
    "  for i, (img, labels) in enumerate(train_loader):\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    imgs = imgs.view(-1, 28 * 28)\n",
    "\n",
    "    outputs = model(imgs)\n",
    "    # out = model(input)\n",
    "    loss = criterion(outputs, labels)\n",
    "    # loss = (ouput, target)을 한 쌍으로 입력 받아 ouput이 target으로부터 얼마나 멀리 떨어져있는지 추정하는 값을 계산한다.\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    # grad를 0으로 초기화 시키는 것이 목적, 전방향 학습, 기울기 초기화\n",
    "    # 이전 step에서서 각 layer별로 계산된 gradient값을 모두 0으로 초기화\n",
    "    # 0으로 초기화 하지 않으면 이전 step의 결과에 현재 step의 gradient가 누적으로 합해져 계산된다.\n",
    "    loss.backward()\n",
    "    # 각 layer의 파라미터에 대한 back-propagation을 통해 gradient를 계산한다.\n",
    "    optimizer.step()\n",
    "    # 각 layer의 파라미터와 같이 저장된 gradient값을 이용해 파라미터를 업데이트한다.\n",
    "    # 파라미터가 업데이트 되면 모델의 성능이 개선된다.\n",
    "    # optimizer는 step()을 통해 argument로 전달받은 parameter를 업데이트한다.\n",
    "\n",
    "    # optimizer를 사용하는 순서는 뉴럴 네트워크 출력값과 라벨값을 loss함수를 이용하여 계산하고,\n",
    "    # 그 loss함수의 .backward()연산을 한 뒤 optimizer.step()을 통해 weight를 업데이트한다.\n",
    "\n",
    "    _,argmax = torch.max(outputs, 1)\n",
    "    accuracy = (labels == argmax).float().mean()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "      print('Epoch [{}/{}], Step [{}/{}], Loss: {: .4f}, Accuracy: {: .2f}%'.format(\n",
    "            epoch+1, training_epochs, i+1, len(train_loader), loss.item(), accuracy.item()* 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
