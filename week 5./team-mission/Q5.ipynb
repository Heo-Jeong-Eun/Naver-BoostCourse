{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. MNIST 데이터셋을 Load -> 학습할 모델을 구현, 가중치 초기화 \n",
    "#     -> 구현한 모델을 학습시키기 위한 loss 함수 opitmizer 구현\n",
    "#     -> 구현한 함수들을 이용해 학습 Loop 구현 \n",
    "#     -> 학습 완료 후 테스트 데이터를 사용해 테스트 진행\n",
    "# output : 테스트 코드\n",
    "\n",
    "# A. \n",
    "#   1. outpus 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    " \n",
    "\n",
    "training_epochs = 15 \n",
    "batch_size = 100 \n",
    "\n",
    "root = './data' \n",
    "mnist_train = dset.MNIST (root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dset.MNIST (root=root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True) \n",
    "test_loader = DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "linear = torch.nn.Linear(784, 10, bias = True).to(device)\n",
    "\n",
    "torch.nn.init.normal_(linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr = 0.1)\n",
    "# lr = learning rate, 미분값을 얼만큼 이동시킬 것인가를 결정 한다.\n",
    "# 초기값이 크다면 초반에는 loss값이 빠르게 줄어들지만 나중에는 underfitting이 발생하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# 다중분류를 위한 손실함수, 다중 분류에 사용된다. \n",
    "# torch.nn.CrossEntropyLoss는 nnLogSoftMax와 nn.NLLLoss 연산의 조합이다. \n",
    "# 배치 처리에 대한 손실값은 배치를 구성하는 각 데이터 손실값의 평균이다. \n",
    "# 연산을 한번에 처리해 수식이 간소화 되어 역전파가 더 안정적으로 이루어지므로 위 코드와 같은 방법이 실제 사용에 권장된다.\n",
    "# Softmax와 Log처리 및 CrossEntropy연산의 조합이다. \n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr = 0.1)\n",
    "# parameter 최적화\n",
    "# 최적화는 각 학습 단계에서 모델의 오류를 줄이기위해 모델 매개변수를 조정하는 과정이다. \n",
    "# 최적화 알고리즘은 이 과정에서 수행되는 방식을 정의한다. \n",
    "# 위 코드의 경우 Stochastic Gradient Descent, 즉 확률적 경사하강법을 정의한다.\n",
    "# 학습하려는 모델의 매개변수와 학습률을 하이퍼파라미터를 등록해 optimizer를 초기화한다.\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(training_epochs):\n",
    "  for i, (imgs, labels) in enumerate(train_loader):\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    imgs = imgs.view(-1, 28 * 28)\n",
    "\n",
    "    outputs = linear(imgs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _,argmax = torch.max(outputs, 1)\n",
    "    accuracy = (labels == argmax).float().mean()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "      print('Epoch [{}/{}], Step [{}/{}], Loss: {: .4f}, Accuracy: {: .2f}%'.format(\n",
    "            epoch+1, training_epochs, i+1, len(train_loader), loss.item(), accuracy.item()* 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        imgs = imgs.view(-1, 28 * 28)\n",
    "\n",
    "        outputs = linear(imgs)\n",
    "\n",
    "        _, argmax = torch.max(outputs, 1) # max()를 통해 최종 출력이 가장 높은 class 선택\n",
    "        total += imgs.size(0)\n",
    "        correct += (labels == argmax). sum().item()\n",
    "\n",
    "    print('Test accuracy for {} images: {: .2f}%'.format(total, correct / total * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
