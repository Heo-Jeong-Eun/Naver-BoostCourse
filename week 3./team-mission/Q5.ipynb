{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. linear regression 구현을 위한 학습 loop 구현, 최종적으로 1000번 반복 결과를 출력한다. \n",
    "# error 차이는 제곱을 이용해 계산, gradient 값은 예측하는 각 변수의 미분값이다. \n",
    "\n",
    "# A. \n",
    "#   1. import numpy\n",
    "#   2. y 예측값을 구하는 식 작성\n",
    "#   3. 오차 error는 y값 - y 예측값\n",
    "#   4. 평균 제곱 오차를 미분한 값\n",
    "#   5. 학습률 * 미분 결과 후 기존 값 업데이트 \n",
    "#   6. epoch가 1000번 반복될 때마다 출력\n",
    "\n",
    "# 경사하강법이란 쉽게말해 오차가 최소화 되는 지점을 찾아가는 과정이라고 할 수 있다. \n",
    "# 오차함수 그래프에서 미분값이 0인 지점을 찾는 것이 경사하강법을 사용하는 궁극적 목표이다. \n",
    "# 경사를 하강시키며 변화시켜주는데 이때 얼만큼 이동시킬지 이동거리를 결정하는 것을 학습률이라고 할 수 있다. \n",
    "# 학습률의 값에 최적이라는 것은 없고, 적절히 바꿔주며 최적의 학습률을 찾아야 한다.\n",
    "# 오차가 최소값이 되는 지점을 찾기 위해서는 이차함수 미분식이 필요하고 평균 제곱 오차를 미분한 결과와 학습률을 곱해 기존 값을 업데이트 한 것을 프로그래밍화 시켜야한다. \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "xy = np.array([[1., 2., 3., 4., 5., 6.], [10., 20., 30., 40., 50., 60.]])\n",
    "\n",
    "x_train = xy[0]\n",
    "y_train = xy[1]\n",
    "\n",
    "beta_gd = np.random.uniform(low=-1.0, high=1.0) # random 값으로 beta_gd를 초기화\n",
    "bias = np.random.uniform(low=-1.0, high=1.0) # random 값으로 bias를 초기화\n",
    "\n",
    "learning_rate = 0.01 \n",
    "# 학습률, 얼만큼 가중치를 업데이트 해야 하는지에 대한 학습량\n",
    "# 학습률이 너무 크면 가중치 갱신이 크게 되어 error가 수렴하지 못하고 발산할 수 있다.\n",
    "# 반대로 학습률이 너무 작으면 가중치 갱신이 충분히 되지 못하고 학습이 끝나는 과소적합이 되어있는 상태로 남아있을 수 있다.\n",
    "\n",
    "for i in range(1000): # 최대반복 횟수\n",
    "  y_pred = beta_gd * x_train + bias # y 예측값을 구하는 식, perdication값으로 가설함수에서 실제값을 뺀 함수를 정의한다.\n",
    "  error = np.abs(y_pred - y_train).mean() # error\n",
    "  # error = ((y_pred - y_train) ** 2).mean() ->  error = np.abs(y_pred - y_train).mean()\n",
    "  # 음수 오차를 허용하지 않기 위해 제곱을 했는데 처음 값이 비정상적으로 크게 나오는 것 같아 abs로 절댓값을 return하도록 했다.\n",
    "\n",
    "  # 평균 제곱 오차를 x_train과 y_train으로 미분한 값을 구하는식 -> 프로그래밍화\n",
    "  # beta_gd_update = (1 / len(x_train)) * sum((x_train) * (error))\n",
    "  # bias_update = (1 / len(x_train)) * sum(y_train - y_pred)\n",
    "\n",
    "  beta_gd = beta_gd - (learning_rate * (y_pred - y_train) * x_train).mean()\n",
    "  bias = bias - (learning_rate * (y_pred - y_train)).mean()\n",
    "  # beta_gd, bias에 미분값 * 학습률을 적용, 기존 값을 바로 업데이트해 저장할 수 있으므로 한번에 계산했다.\n",
    "  \n",
    "  if i % 100 == 0:\n",
    "    print(\"Epoch ({:10d}/1000) error: {:10f}, beta_gd: {:10f}, bias: {:10f}\".format(i, error, beta_gd.item(), bias.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
